 
Exp 5 : Comparative Analysis of different types of Prompting patterns and explain with 
Various Test Scenarios 
AIM: 
Test and compare how different pattern models respond to various prompts (broad or 
unstructured) versus basic prompts (clearer and more refined) across multiple scenarios.  
Analyze the quality, accuracy, and depth of the generated responses. 
AI TOOLS REQUIRED: 
o For assistance- ChatGpt, Claude, DeepSeek 
o For Video Generation- Runway 
o For Image Generation- Midjourney 
Procedure:  
1. Objective: 
o This analysis seeks to evaluate the responses of various AI models to different levels of 
prompt clarity. By comparing naïve (broad, unstructured) prompts to basic (clearer, more 
refined) prompts, we aim to understand how prompt precision influences the quality, 
accuracy, and relevance of AI-generated responses. 
 
Recognizing how prompt specificity impacts AI responses is essential for users seeking 
optimal model performance across various tasks, including factual responses, creative output, 
and technical explanations 
o Basic Prompts: Basic prompts are clearer and provide essential context, giving the model 
specific direction on desired content. For example, “Provide a brief overview of climate 
change, covering its causes, major impacts, and potential solutions” offers a structured focus 
that the model can directly follow. 
o Evaluation Metrics: Key metrics for assessment include quality (how well the response 
addresses the prompt), accuracy (factual correctness), depth (level of detail), relevance 
(response alignmentwith prompt), conciseness, and adaptability (the model's ability to 
infer missing context or handle vague requests). 
Methodology 
1. Models Selection: 
o Selected models include GPT-3.5, GPT-4, Claude, and a leading open-source model. These 
choices reflect a range of capabilities: GPT-4 for depth and contextual handling, Claude for 
precision and conversational tone, and an open-source model to assess adaptability with less 
Saveetha Engineering College 
refined prompt engineering. 
2. Prompt Scenarios: 
o Five core scenarios were chosen: summarization (e.g., summarizing an article), complex 
query answering (e.g., explaining a scientific concept), creative writing (e.g., storytelling), 
technical explanation (e.g., explaining a code snippet), and opinion-based inquiry (e.g., pros 
and cons of remote work). 
o Each scenario was tested with a naïve prompt (e.g., “Explain photosynthesis”) and a basic 
prompt (e.g., “Describe the photosynthesis process, highlighting the roles of chlorophyll, 
sunlight, water, and carbon dioxide”). 
3. Evaluation Metrics: 
o Responses were assessed on a 5-point scale across each metric: quality, accuracy, depth, 
relevance, conciseness, and adaptability. Qualitative notes complemented scores to capture 
nuances in model behaviour. 
Analysis of Basic Prompts 
1. Scenario Results: 
o With basic prompts, models delivered more targeted, structured responses. For example, 
when asked, “Explain photosynthesis by focusing on chlorophyll and energy transformation,” 
GPT-4 produced an in-depth answer that maintained focus, while Claude generated a 
succinct, clear explanation. Even open-source models improved, delivering responses more 
aligned with the prompt’s requirements. 
2. Observations: 
o Improvements: Basic prompts markedly enhanced depth, accuracy, and alignment with the 
intended focus. Responses were consistently higher quality and demonstrated better adherence 
to the prompt structure. 
o Model-Specific Benefits: GPT-4 showcased the most substantial improvement in depth and 
relevance, effectively addressing all prompt components. Claude exhibited clear, concise 
answers suitable for simpler explanations. Open-source models showed marginal 
improvement, though remained less accurate than proprietary models. 
o Detailed Comparison: With refined prompts, GPT-4 generated responses of greater depth 
and accuracy, while GPT-3.5 offered quicker, more concise responses that suited 
straightforward tasks. Claude’s responses were consistently accurate but leaned towards 
conversational summaries, suitable for non-specialist audiences. 
 
Key Findings and Insights 
1. Prompt Sensitivity: 
o GPT-4 demonstrated high sensitivity to prompt clarity, delivering accurate and in-depth 
responses when given basic prompts. Claude also benefited, offering clearer explanations 
Saveetha Engineering College 
with basic prompts but struggling with inferential depth on naïve prompts. Open-source 
models were less responsive to prompt refinement, suggesting limitations in inference 
capabilities. 
2. Efficiency and Practicality: 
o Refining prompts proved especially practical for tasks requiring depth and specificity, as it 
reduced the time needed to clarify responses. GPT-3.5 and Claude consistently provided 
efficient, relevant answers with concise prompts, beneficial in time-sensitive scenarios. 
3. Contextual Adaptability: 
o While GPT-4 adapted to inferred context best, it also occasionally over-explained on naïve 
prompts. Claude’s adaptability shone in conversational and practical summaries but lacked 
depth on more technical prompts. Open-source models were the least adaptable, suggesting a 
need for more explicit guidance. 
 
 
Conclusion and Recommendations 
1. Summary of Findings: 
o Prompt clarity significantly impacts response quality across AI models, with GPT-4 showing 
the most noticeable improvement in depth and relevance. While all models improved with 
basic prompts, open-source models exhibited the least benefit from prompt refinement. 
2. Best Practices: 
For optimal results, users should provide clear, structured prompts, detailing scope and 
focus to guide models more effectively. This ensures higher accuracy and alignment with 
user intent, particularly for complex or nuanced queries. 
 
 
 
RESULT: 
Thus, the comparative analysis of different types of prompting patterns reveals significant 
variation in performance depending on the complexity of the task and the structure of the 
prompt.
